{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Removing HTML tags\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text_data):\n",
    " soup = BeautifulSoup(text_data, \"html.parser\")\n",
    " #[s.extract() for s in soup(['iframe', 'script'])]\n",
    " text = soup.get_text()\n",
    " return text"
   ]
  },
  {
   "source": [
    "Removing Accented Characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accent_chars(text_data):\n",
    "        text_data = unicodedata.normalize('NFKD', text_data).encode('ascii',          'ignore').decode('utf-8', 'ignore')\n",
    "        return text_data"
   ]
  },
  {
   "source": [
    "Text Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text_data):\n",
    "    sentences = nltk.sent_tokenize(text_data)\n",
    "    token_words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return token_words"
   ]
  },
  {
   "source": [
    "Removing special characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text_data, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text_data = re.sub(pattern, '', text_data)\n",
    "    return text_data"
   ]
  },
  {
   "source": [
    "Remove repeated characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_repeated_chars(tokens):\n",
    "   pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "   match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "   def replace(old_w):  ## old word\n",
    "      if wordnet.synsets(old_w):\n",
    "         return old_w\n",
    "      new_w = pattern.sub(match_substitution, old_w) # new word\n",
    "      return replace(new_w) if new_w != old_w else new_w\n",
    "      correct_tokens = [replace(w) for w in tokens]  #word\n",
    "      return correct_tokens"
   ]
  },
  {
   "source": [
    "Spelle corrector algorithms\n",
    "\n",
    "1. TextBlob\n",
    "2. PyEnchant\n",
    "3. Auto correct\n",
    "4. aspell-python\n",
    "5. Deep learning based DeepSpell"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemmer(text_data):\n",
    " ps = nltk.stem.porter.PorterStemmer()\n",
    " text_data = ' '.join([ps.stem(w) for w in text_data.split()])\n",
    " return text_data"
   ]
  },
  {
   "source": [
    "Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "def text_lemmatizer(text_data):\n",
    "    text_data = nlp(text_data)\n",
    "    text_data = ' '.join([w.lemma_ if word.lemma_ != '-PRON-' else w.text\n",
    "    for w in text_data])\n",
    "    return text_data"
   ]
  },
  {
   "source": [
    "Removing Stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text_data, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text_data)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in\n",
    "        stopwords_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not\n",
    "    in stopwords_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}