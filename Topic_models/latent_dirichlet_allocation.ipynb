{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import LdaModel,Phrases,phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n"
   ]
  },
  {
   "source": [
    "Data retrival from current directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['nips12', 'nips02', 'nips04', 'nips00', 'MATLAB_NOTES', 'idx', 'README_yann', 'nips05', 'nips08', 'nips06', 'nips07', 'RAW_DATA_NOTES', 'nips03', 'nips09', 'orig', 'nips01', 'nips11', 'nips10']\n"
     ]
    }
   ],
   "source": [
    "DataPath = 'nipstxt/'\n",
    "print(os.listdir(DataPath))"
   ]
  },
  {
   "source": [
    "Load and view dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "#Read all texts into a list\n",
    "research_papers = []\n",
    "for folder in folders:\n",
    "    f_names  = os.listdir(DataPath+folder) # file names\n",
    "    for f_name in f_names:\n",
    "        with open(DataPath+folder+'/'+f_name, encoding='utf-8',errors='ignore',mode='r+') as f:\n",
    "            data = f.read()\n",
    "            research_papers.append(data)\n",
    "#Total number of research papers in the corpora\n",
    "len(research_papers)"
   ]
  },
  {
   "source": [
    "Preprocessing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1740\nCPU times: user 30.2 s, sys: 172 ms, total: 30.4 s\nWall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stop_words = stopwords.words('english')\n",
    "word_tk = RegexpTokenizer(r'\\w+')\n",
    "word_nl = WordNetLemmatizer()\n",
    "\n",
    "def norm_corpus(papers):\n",
    "    # Storing normalized papers\n",
    "    normalize_papers = []\n",
    "    for paper in papers:\n",
    "        # Lowercasing text\n",
    "        paper = paper.lower()\n",
    "        # Tokenizing the text\n",
    "        paper_tokens = [token.strip() for token in word_tk.tokenize(paper)]\n",
    "        # lemmatization of text\n",
    "        paper_tokens = [word_nl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        # length of each token is greater than 1\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        # Removing the stopwords\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None,paper_tokens))\n",
    "        if paper_tokens:\n",
    "            normalize_papers.append(paper_tokens)\n",
    "    return normalize_papers\n",
    "\n",
    "normalize_papers = norm_corpus(research_papers)\n",
    "print(len(normalize_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['presynaptic',\n",
       " 'neural',\n",
       " 'information',\n",
       " 'processing',\n",
       " 'carley',\n",
       " 'department',\n",
       " 'electrical',\n",
       " 'computer',\n",
       " 'engineering',\n",
       " 'carnegie',\n",
       " 'mellon',\n",
       " 'university',\n",
       " 'pittsburgh',\n",
       " 'pa',\n",
       " 'abstract',\n",
       " 'potential',\n",
       " 'presynaptic',\n",
       " 'information',\n",
       " 'processing',\n",
       " 'within',\n",
       " 'arbor',\n",
       " 'single',\n",
       " 'axon',\n",
       " 'discussed',\n",
       " 'paper',\n",
       " 'current',\n",
       " 'knowledge',\n",
       " 'activity',\n",
       " 'dependence',\n",
       " 'firing',\n",
       " 'threshold',\n",
       " 'condition',\n",
       " 'required',\n",
       " 'conduction',\n",
       " 'failure',\n",
       " 'similarity',\n",
       " 'node',\n",
       " 'along',\n",
       " 'single',\n",
       " 'axon']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "normalize_papers[1][0:40]"
   ]
  },
  {
   "source": [
    "Topic models with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Text representation with feature engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['scaling',\n",
       " 'property',\n",
       " 'coarse_coded',\n",
       " 'symbol',\n",
       " 'memory',\n",
       " 'ronald',\n",
       " 'rosenfeld',\n",
       " 'david_touretzky',\n",
       " 'computer_science',\n",
       " 'department',\n",
       " 'carnegie_mellon',\n",
       " 'university_pittsburgh',\n",
       " 'pennsylvania',\n",
       " 'abstract',\n",
       " 'coarse_coded',\n",
       " 'symbol',\n",
       " 'memory',\n",
       " 'appeared',\n",
       " 'several',\n",
       " 'neural_network',\n",
       " 'symbol',\n",
       " 'processing',\n",
       " 'model',\n",
       " 'order',\n",
       " 'determine',\n",
       " 'model',\n",
       " 'would',\n",
       " 'scale',\n",
       " 'one',\n",
       " 'must',\n",
       " 'first',\n",
       " 'understanding',\n",
       " 'mathematics']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "bigram = Phrases(normalize_papers,min_count=25,threshold=25,delimiter=b'_',)\n",
    "bigram_model = phrases.Phraser(bigram)\n",
    "\n",
    "#sample bigram features\n",
    "bigram_model[normalize_papers[0][0:40]]"
   ]
  },
  {
   "source": [
    "Let's obtain a unique term or phrase to number mappings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word to number mappings [(0, '8a'), (1, 'abandon'), (2, 'able'), (3, 'abo'), (4, 'abstract'), (5, 'accommodate'), (6, 'accuracy'), (7, 'achieved'), (8, 'acknowledgment_thank'), (9, 'across')]\nSize of vocabulary 78112\n"
     ]
    }
   ],
   "source": [
    "normalize_corpus_bigrams = [bigram_model[doc] for doc in normalize_papers]\n",
    "\n",
    "# Dictionary representation of the documents\n",
    "dictionary = Dictionary(normalize_corpus_bigrams)\n",
    "print('word to number mappings',list(dictionary.items())[0:10])\n",
    "print('Size of vocabulary',len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of vocabulary 7512\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20,no_above=0.6)\n",
    "print('Size of vocabulary', len(dictionary)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 6), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bags of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in normalize_corpus_bigrams]\n",
    "print(bow_corpus[0][0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('able', 1), ('accommodate', 1), ('accuracy', 1), ('achieved', 1), ('acknowledgment_thank', 1), ('across', 1), ('active', 6), ('activity', 1), ('actual', 1), ('adjusted', 1), ('adjusting', 2), ('agrees', 1), ('algebraic', 1), ('allow', 1), ('along', 1), ('alphabet', 2), ('alternative', 1), ('alternatively', 1), ('although', 2), ('american_institute', 1)]\nTotal number of papers 1740\n"
     ]
    }
   ],
   "source": [
    "# Viewing the actual terms and their counts\n",
    "print([(dictionary[idx],freq) for idx, freq in bow_corpus[0][0:20]])\n",
    "\n",
    "# Total number of papers in the corpus\n",
    "print('Total number of papers',len(bow_corpus))"
   ]
  },
  {
   "source": [
    "Latent Dirichelt Allocation (LDA)\n",
    "\n",
    "It's a generative probabilistic model in which each document is assumed to have a combination of topics similar to probabilistic Latent Semantic Index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2min 34s, sys: 66.8 ms, total: 2min 34s\nWall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_topics = 5\n",
    "lda_bow = LdaModel(bow_corpus,id2word=dictionary,num_topics=total_topics,passes=10,chunksize=1740,iterations=400)  "
   ]
  },
  {
   "source": [
    "View the major topics or themes in corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #1:\n0.013*\"image\" + 0.007*\"feature\" + 0.006*\"recognition\" + 0.006*\"object\" + 0.005*\"task\" + 0.004*\"signal\" + 0.004*\"word\" + 0.004*\"visual\" + 0.004*\"human\" + 0.004*\"trained\" + 0.004*\"unit\" + 0.004*\"speech\" + 0.004*\"classification\" + 0.003*\"representation\" + 0.003*\"position\"\n\nTopic #2:\n0.016*\"neuron\" + 0.012*\"cell\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"activity\" + 0.006*\"pattern\" + 0.004*\"unit\" + 0.004*\"dynamic\" + 0.004*\"control\" + 0.004*\"signal\" + 0.004*\"spike\" + 0.004*\"synaptic\" + 0.004*\"layer\" + 0.003*\"visual\" + 0.003*\"connection\"\n\nTopic #3:\n0.015*\"unit\" + 0.007*\"pattern\" + 0.007*\"vector\" + 0.006*\"state\" + 0.006*\"net\" + 0.005*\"layer\" + 0.005*\"hidden_unit\" + 0.005*\"rule\" + 0.005*\"node\" + 0.004*\"sequence\" + 0.004*\"architecture\" + 0.004*\"memory\" + 0.004*\"activation\" + 0.004*\"signal\" + 0.003*\"representation\"\n\nTopic #4:\n0.012*\"state\" + 0.005*\"action\" + 0.004*\"class\" + 0.004*\"probability\" + 0.004*\"let\" + 0.004*\"optimal\" + 0.004*\"step\" + 0.004*\"linear\" + 0.004*\"vector\" + 0.003*\"control\" + 0.003*\"size\" + 0.003*\"theorem\" + 0.003*\"bound\" + 0.003*\"node\" + 0.003*\"policy\"\n\nTopic #5:\n0.007*\"distribution\" + 0.006*\"image\" + 0.005*\"feature\" + 0.005*\"vector\" + 0.005*\"gaussian\" + 0.004*\"probability\" + 0.004*\"class\" + 0.004*\"variable\" + 0.004*\"estimate\" + 0.003*\"noise\" + 0.003*\"linear\" + 0.003*\"prior\" + 0.003*\"matrix\" + 0.003*\"approximation\" + 0.003*\"sample\"\n\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_bow.print_topics(num_topics=8, num_words=15):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "source": [
    "Overall mean coherence score of the model is used measure quality of topic models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average coherence score: -0.9276235007333614\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_bow.top_topics(bow_corpus,topn=15)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Average coherence score:',avg_coherence_score)"
   ]
  },
  {
   "source": [
    "Perplexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model perplexity : -7.868744647919709\n"
     ]
    }
   ],
   "source": [
    "perplexity = lda_bow.log_perplexity(bow_corpus)\n",
    "print('Model perplexity :',perplexity)"
   ]
  }
 ]
}