{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import LsiModel,Phrases,phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n"
   ]
  },
  {
   "source": [
    "Data retrival from current directory"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['nips12', 'nips02', 'nips04', 'nips00', 'MATLAB_NOTES', 'idx', 'README_yann', 'nips05', 'nips08', 'nips06', 'nips07', 'RAW_DATA_NOTES', 'nips03', 'nips09', 'orig', 'nips01', 'nips11', 'nips10']\n"
     ]
    }
   ],
   "source": [
    "DataPath = 'nipstxt/'\n",
    "print(os.listdir(DataPath))"
   ]
  },
  {
   "source": [
    "Load and view dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "#Read all texts into a list\n",
    "research_papers = []\n",
    "for folder in folders:\n",
    "    f_names  = os.listdir(DataPath+folder) # file names\n",
    "    for f_name in f_names:\n",
    "        with open(DataPath+folder+'/'+f_name, encoding='utf-8',errors='ignore',mode='r+') as f:\n",
    "            data = f.read()\n",
    "            research_papers.append(data)\n",
    "#Total number of research papers in the corpora\n",
    "len(research_papers)"
   ]
  },
  {
   "source": [
    "Preprocessing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1740\nCPU times: user 31.4 s, sys: 185 ms, total: 31.6 s\nWall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stop_words = stopwords.words('english')\n",
    "word_tk = RegexpTokenizer(r'\\w+')\n",
    "word_nl = WordNetLemmatizer()\n",
    "\n",
    "def norm_corpus(papers):\n",
    "    # Storing normalized papers\n",
    "    normalize_papers = []\n",
    "    for paper in papers:\n",
    "        # Lowercasing text\n",
    "        paper = paper.lower()\n",
    "        # Tokenizing the text\n",
    "        paper_tokens = [token.strip() for token in word_tk.tokenize(paper)]\n",
    "        # lemmatization of text\n",
    "        paper_tokens = [word_nl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        # length of each token is greater than 1\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        # Removing the stopwords\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None,paper_tokens))\n",
    "        if paper_tokens:\n",
    "            normalize_papers.append(paper_tokens)\n",
    "    return normalize_papers\n",
    "\n",
    "normalize_papers = norm_corpus(research_papers)\n",
    "print(len(normalize_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['presynaptic',\n",
       " 'neural',\n",
       " 'information',\n",
       " 'processing',\n",
       " 'carley',\n",
       " 'department',\n",
       " 'electrical',\n",
       " 'computer',\n",
       " 'engineering',\n",
       " 'carnegie',\n",
       " 'mellon',\n",
       " 'university',\n",
       " 'pittsburgh',\n",
       " 'pa',\n",
       " 'abstract',\n",
       " 'potential',\n",
       " 'presynaptic',\n",
       " 'information',\n",
       " 'processing',\n",
       " 'within',\n",
       " 'arbor',\n",
       " 'single',\n",
       " 'axon',\n",
       " 'discussed',\n",
       " 'paper',\n",
       " 'current',\n",
       " 'knowledge',\n",
       " 'activity',\n",
       " 'dependence',\n",
       " 'firing',\n",
       " 'threshold',\n",
       " 'condition',\n",
       " 'required',\n",
       " 'conduction',\n",
       " 'failure',\n",
       " 'similarity',\n",
       " 'node',\n",
       " 'along',\n",
       " 'single',\n",
       " 'axon',\n",
       " 'reviewed',\n",
       " 'electronic',\n",
       " 'circuit',\n",
       " 'model',\n",
       " 'site',\n",
       " 'low',\n",
       " 'conduction',\n",
       " 'safety',\n",
       " 'axon',\n",
       " 'presented']"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "normalize_papers[1][0:40]"
   ]
  },
  {
   "source": [
    "Topic models with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Text representation with feature engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['scaling',\n",
       " 'property',\n",
       " 'coarse_coded',\n",
       " 'symbol',\n",
       " 'memory',\n",
       " 'ronald',\n",
       " 'rosenfeld',\n",
       " 'david_touretzky',\n",
       " 'computer_science',\n",
       " 'department',\n",
       " 'carnegie_mellon',\n",
       " 'university_pittsburgh',\n",
       " 'pennsylvania',\n",
       " 'abstract',\n",
       " 'coarse_coded',\n",
       " 'symbol',\n",
       " 'memory',\n",
       " 'appeared',\n",
       " 'several',\n",
       " 'neural_network',\n",
       " 'symbol',\n",
       " 'processing',\n",
       " 'model',\n",
       " 'order',\n",
       " 'determine',\n",
       " 'model',\n",
       " 'would',\n",
       " 'scale',\n",
       " 'one',\n",
       " 'must',\n",
       " 'first',\n",
       " 'understanding',\n",
       " 'mathematics']"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "bigram = Phrases(normalize_papers,min_count=25,threshold=25,delimiter=b'_',)\n",
    "bigram_model = phrases.Phraser(bigram)\n",
    "\n",
    "#sample bigram features\n",
    "bigram_model[normalize_papers[0][0:40]]"
   ]
  },
  {
   "source": [
    "Let's obtain a unique term or phrase to number mappings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word to number mappings [(0, '8a'), (1, 'abandon'), (2, 'able'), (3, 'abo'), (4, 'abstract'), (5, 'accommodate'), (6, 'accuracy'), (7, 'achieved'), (8, 'acknowledgment_thank'), (9, 'across')]\nSize of vocabulary 78112\n"
     ]
    }
   ],
   "source": [
    "normalize_corpus_bigrams = [bigram_model[doc] for doc in normalize_papers]\n",
    "\n",
    "# Dictionary representation of the documents\n",
    "dictionary = Dictionary(normalize_corpus_bigrams)\n",
    "print('word to number mappings',list(dictionary.items())[0:10])\n",
    "print('Size of vocabulary',len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of vocabulary 7512\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20,no_above=0.6)\n",
    "print('Size of vocabulary', len(dictionary)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 6), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bags of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in normalize_corpus_bigrams]\n",
    "print(bow_corpus[0][0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('able', 1), ('accommodate', 1), ('accuracy', 1), ('achieved', 1), ('acknowledgment_thank', 1), ('across', 1), ('active', 6), ('activity', 1), ('actual', 1), ('adjusted', 1), ('adjusting', 2), ('agrees', 1), ('algebraic', 1), ('allow', 1), ('along', 1), ('alphabet', 2), ('alternative', 1), ('alternatively', 1), ('although', 2), ('american_institute', 1)]\nTotal number of papers 1740\n"
     ]
    }
   ],
   "source": [
    "# Viewing the actual terms and their counts\n",
    "print([(dictionary[idx],freq) for idx, freq in bow_corpus[0][0:20]])\n",
    "\n",
    "# Total number of papers in the corpus\n",
    "print('Total number of papers',len(bow_corpus))"
   ]
  },
  {
   "source": [
    "Latent Semantic Index (LSI)\n",
    "\n",
    "It's a statistical technique to correlate semantically terms to form topics. It's uses Singular Value Decomposition(SVD) technique."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 22min 25s, sys: 5min 36s, total: 28min 1s\nWall time: 16min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_topics = 8\n",
    "lsi_bow = LsiModel(bow_corpus,id2word=dictionary,num_topics=total_topics,onepass=True,chunksize=1740,power_iters=1200)  "
   ]
  },
  {
   "source": [
    "View the major topics or themes in corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #1:\n0.215*\"unit\" + 0.214*\"state\" + 0.180*\"neuron\" + 0.160*\"pattern\" + 0.151*\"image\" + 0.139*\"vector\" + 0.132*\"feature\" + 0.127*\"cell\" + 0.109*\"layer\" + 0.102*\"probability\" + 0.100*\"task\" + 0.100*\"distribution\" + 0.095*\"class\" + 0.090*\"rate\" + 0.089*\"signal\"\n\nTopic #2:\n-0.455*\"neuron\" + -0.399*\"cell\" + 0.340*\"state\" + -0.189*\"response\" + -0.170*\"stimulus\" + 0.117*\"action\" + -0.116*\"activity\" + -0.107*\"pattern\" + 0.096*\"class\" + 0.095*\"vector\" + -0.094*\"visual\" + -0.093*\"spike\" + -0.093*\"synaptic\" + -0.090*\"circuit\" + 0.089*\"probability\"\n\nTopic #3:\n0.539*\"state\" + -0.464*\"image\" + 0.287*\"neuron\" + -0.243*\"feature\" + 0.172*\"action\" + -0.168*\"object\" + 0.116*\"control\" + -0.112*\"unit\" + -0.098*\"recognition\" + -0.095*\"classifier\" + -0.094*\"class\" + 0.092*\"policy\" + 0.083*\"cell\" + -0.080*\"classification\" + 0.080*\"dynamic\"\n\nTopic #4:\n0.746*\"unit\" + -0.235*\"image\" + -0.187*\"neuron\" + 0.153*\"pattern\" + 0.126*\"layer\" + 0.125*\"net\" + 0.122*\"hidden_unit\" + 0.114*\"activation\" + -0.112*\"distribution\" + 0.094*\"rule\" + -0.093*\"class\" + -0.071*\"sample\" + -0.070*\"linear\" + 0.067*\"word\" + 0.066*\"connection\"\n\nTopic #5:\n-0.486*\"image\" + -0.410*\"state\" + -0.188*\"object\" + -0.183*\"action\" + 0.155*\"class\" + 0.138*\"distribution\" + -0.134*\"visual\" + -0.131*\"control\" + 0.131*\"neuron\" + 0.125*\"classifier\" + 0.123*\"node\" + 0.121*\"vector\" + -0.110*\"task\" + -0.108*\"feature\" + -0.107*\"cell\"\n\nTopic #6:\n0.638*\"cell\" + -0.514*\"neuron\" + -0.156*\"image\" + 0.117*\"distribution\" + -0.110*\"chip\" + -0.104*\"net\" + -0.094*\"circuit\" + -0.093*\"object\" + 0.085*\"response\" + 0.078*\"rat\" + 0.076*\"probability\" + -0.075*\"feature\" + -0.075*\"memory\" + -0.074*\"word\" + -0.073*\"recognition\"\n\nTopic #7:\n0.333*\"word\" + 0.292*\"classifier\" + -0.284*\"unit\" + 0.215*\"feature\" + 0.200*\"cell\" + 0.186*\"pattern\" + 0.178*\"class\" + -0.177*\"image\" + -0.176*\"noise\" + 0.173*\"recognition\" + -0.159*\"distribution\" + 0.144*\"node\" + 0.137*\"classification\" + -0.120*\"gaussian\" + -0.104*\"motion\"\n\nTopic #8:\n-0.310*\"word\" + 0.246*\"pattern\" + -0.245*\"signal\" + 0.222*\"neuron\" + 0.220*\"rule\" + -0.200*\"control\" + 0.183*\"image\" + 0.174*\"state\" + -0.159*\"circuit\" + -0.139*\"noise\" + -0.132*\"task\" + -0.123*\"motion\" + 0.117*\"feature\" + -0.115*\"target\" + -0.111*\"current\"\n\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lsi_bow.print_topics(num_topics=8, num_words=15):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "source": [
    "We can observe that terms or words and assined weights have positive or negative sign. We are going to separate posive and negative weights corresponding terms. Higher the weight, more important the contribution. Each term indicates a sign of direction or orientation in the vector space for a particualr topic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #1:\n----------------------------------------------------------------\nPositive Direction: [('unit', 0.22), ('state', 0.21), ('neuron', 0.18), ('pattern', 0.16), ('image', 0.15), ('vector', 0.14), ('feature', 0.13), ('cell', 0.13), ('layer', 0.11), ('probability', 0.1), ('task', 0.1), ('distribution', 0.1), ('class', 0.09), ('rate', 0.09), ('signal', 0.09)]\n----------------------------------------------------------------\nNegative Direction: []\n\nTopic #2:\n----------------------------------------------------------------\nPositive Direction: [('state', 0.34), ('action', 0.12), ('class', 0.1), ('vector', 0.09), ('probability', 0.09)]\n----------------------------------------------------------------\nNegative Direction: [('neuron', -0.45), ('cell', -0.4), ('response', -0.19), ('stimulus', -0.17), ('activity', -0.12), ('pattern', -0.11), ('visual', -0.09), ('spike', -0.09), ('synaptic', -0.09), ('circuit', -0.09)]\n\nTopic #3:\n----------------------------------------------------------------\nPositive Direction: [('state', 0.54), ('neuron', 0.29), ('action', 0.17), ('control', 0.12), ('policy', 0.09), ('cell', 0.08), ('dynamic', 0.08)]\n----------------------------------------------------------------\nNegative Direction: [('image', -0.46), ('feature', -0.24), ('object', -0.17), ('unit', -0.11), ('recognition', -0.1), ('classifier', -0.1), ('class', -0.09), ('classification', -0.08)]\n\nTopic #4:\n----------------------------------------------------------------\nPositive Direction: [('unit', 0.75), ('pattern', 0.15), ('layer', 0.13), ('net', 0.12), ('hidden_unit', 0.12), ('activation', 0.11), ('rule', 0.09), ('word', 0.07), ('connection', 0.07)]\n----------------------------------------------------------------\nNegative Direction: [('image', -0.23), ('neuron', -0.19), ('distribution', -0.11), ('class', -0.09), ('sample', -0.07), ('linear', -0.07)]\n\nTopic #5:\n----------------------------------------------------------------\nPositive Direction: [('class', 0.16), ('distribution', 0.14), ('neuron', 0.13), ('classifier', 0.13), ('node', 0.12), ('vector', 0.12)]\n----------------------------------------------------------------\nNegative Direction: [('image', -0.49), ('state', -0.41), ('object', -0.19), ('action', -0.18), ('visual', -0.13), ('control', -0.13), ('task', -0.11), ('feature', -0.11), ('cell', -0.11)]\n\nTopic #6:\n----------------------------------------------------------------\nPositive Direction: [('cell', 0.64), ('distribution', 0.12), ('response', 0.09), ('rat', 0.08), ('probability', 0.08)]\n----------------------------------------------------------------\nNegative Direction: [('neuron', -0.51), ('image', -0.16), ('chip', -0.11), ('net', -0.1), ('circuit', -0.09), ('object', -0.09), ('feature', -0.07), ('memory', -0.07), ('word', -0.07), ('recognition', -0.07)]\n\nTopic #7:\n----------------------------------------------------------------\nPositive Direction: [('word', 0.33), ('classifier', 0.29), ('feature', 0.22), ('cell', 0.2), ('pattern', 0.19), ('class', 0.18), ('recognition', 0.17), ('node', 0.14), ('classification', 0.14)]\n----------------------------------------------------------------\nNegative Direction: [('unit', -0.28), ('image', -0.18), ('noise', -0.18), ('distribution', -0.16), ('gaussian', -0.12), ('motion', -0.1)]\n\nTopic #8:\n----------------------------------------------------------------\nPositive Direction: [('pattern', 0.25), ('neuron', 0.22), ('rule', 0.22), ('image', 0.18), ('state', 0.17), ('feature', 0.12)]\n----------------------------------------------------------------\nNegative Direction: [('word', -0.31), ('signal', -0.25), ('control', -0.2), ('circuit', -0.16), ('noise', -0.14), ('task', -0.13), ('motion', -0.12), ('target', -0.11), ('current', -0.11)]\n\n"
     ]
    }
   ],
   "source": [
    "for n in range(total_topics):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('-'*64)\n",
    "    p_d = []\n",
    "    n_d = []\n",
    "    for term,weight in lsi_bow.show_topic(n, topn=15):\n",
    "        if weight >= 0:\n",
    "            p_d.append((term,round(weight,2)))\n",
    "        else:\n",
    "            n_d.append((term,round(weight,2)))\n",
    "    print('Positive Direction:',p_d)\n",
    "    print('-'*64)\n",
    "    print('Negative Direction:',n_d)\n",
    "    print()\n"
   ]
  },
  {
   "source": [
    "Let's try to get 3 major metrics like left singular vectors(U), singular values(S) and right singular vectors(VT)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((7512, 8), (8,), (8, 1740))"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "document_topic = (corpus2dense(lsi_bow[bow_corpus],len(singular_values)).T/ singular_values).T \n",
    "\n",
    "term_topic.shape, singular_values.shape, document_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      T1     T2     T3     T4     T5     T6     T7     T8\n",
       "0  0.039  0.004 -0.007  0.077  0.028 -0.012 -0.005  0.059\n",
       "1  0.024 -0.034  0.014 -0.007  0.009  0.009  0.009 -0.020\n",
       "2  0.010 -0.003  0.009 -0.011  0.007 -0.017 -0.011  0.003\n",
       "3  0.025 -0.047 -0.006  0.003 -0.020  0.044  0.007  0.008\n",
       "4  0.015 -0.021  0.013  0.002  0.010 -0.014 -0.001  0.005"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T1</th>\n      <th>T2</th>\n      <th>T3</th>\n      <th>T4</th>\n      <th>T5</th>\n      <th>T6</th>\n      <th>T7</th>\n      <th>T8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.039</td>\n      <td>0.004</td>\n      <td>-0.007</td>\n      <td>0.077</td>\n      <td>0.028</td>\n      <td>-0.012</td>\n      <td>-0.005</td>\n      <td>0.059</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.024</td>\n      <td>-0.034</td>\n      <td>0.014</td>\n      <td>-0.007</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>0.009</td>\n      <td>-0.020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.010</td>\n      <td>-0.003</td>\n      <td>0.009</td>\n      <td>-0.011</td>\n      <td>0.007</td>\n      <td>-0.017</td>\n      <td>-0.011</td>\n      <td>0.003</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.025</td>\n      <td>-0.047</td>\n      <td>-0.006</td>\n      <td>0.003</td>\n      <td>-0.020</td>\n      <td>0.044</td>\n      <td>0.007</td>\n      <td>0.008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.015</td>\n      <td>-0.021</td>\n      <td>0.013</td>\n      <td>0.002</td>\n      <td>0.010</td>\n      <td>-0.014</td>\n      <td>-0.001</td>\n      <td>0.005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "#We can transpose topic document matrix to form document-topic matrix\n",
    "document_topics = pd.DataFrame(np.round(document_topic.T,3),columns=['T'+str(i) for i in range(1,total_topics+1)])\n",
    "document_topics.head()"
   ]
  },
  {
   "source": [
    "The important topics for a few research papers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Document #15:\nDominant topics(top3 ['T7', 'T1', 'T4', 'T2', 'T3']\nPaper summary\n474 \nOPTIMIZATION WITH ARTIFICIAL NEURAL NETWORK SYSTEMS: \nA MAPPING PRINCIPLE \nAND \nA COMPARISON TO GRADIENT BASED METHODS * \nHarrison MonFook Leong \nResearch Institute for Advanced Computer Science \nNASA Ames Research Center 230-5 \nMoffett Field, CA, 94035 \nABSTRACT \nGeneral formulae for mapping optimization problems into systems of ordinary differential \nequations associated with artificial neural networks are presented. A comparison is made to optim- \nization using gradient-search methods. The performance measure is the settling time from an initial \nstate to a target state. A simple analy\n\nDocument #240:\nDominant topics(top3 ['T2', 'T4', 'T6', 'T1', 'T8']\nPaper summary\nNote on Development of Modularity in Simple Cortical Models 133 \nNote \non Development of Modularity \nin Simple Cortical Models \nAlex Chernjavsky 1 \nNeuroscience Graduate Program \nSection of Molecular Neurobiology \nHoward Hughes Medical Institute \nYale University \nJohn Moody ' \nYale Computer Science \nPO Box 2158 Yale Station \nNew Haven, CT 06520 \nEmail: moody@cs.yale.edu \nABSTRACT \nThe existence of modularity in the organization of nervous systems \n(e.g. cortical columns and olfactory glomeruli) is well known. We \nshow that localized activity patterns in a layer of cells, collective \nexcitatio\n\nDocument #450:\nDominant topics(top3 ['T4', 'T7', 'T1', 'T5', 'T8']\nPaper summary\nConstructing Proofs in Symmetric Networks \nGadi Pinkas \nComputer Science Department \nWashington University \nCampus Box 1045 \nSt. Louis, MO 63130 \nAbstract \nThis paper considers the problem of expressing predicate calculus in con- \nnectionist networks that are based on energy minimization. Given a first- \norder-logic knowledge base and a bound k, a symmetric network is con- \nstructed (like a Boltzman machine or a Hopfield network) that searches \nfor a proof for a given query. If a resolution-based proof of length no \nlonger than k exists, then the global minima of the energy function that \nis a\n\nDocument #600:\nDominant topics(top3 ['T8', 'T5', 'T1', 'T2', 'T6']\nPaper summary\nSelf-Organizing Rules for Robust \nPrincipal Component Analysis \nLei Xu,2*and Alan Yuille  \n1. Division of Applied Sciences, Harvard University, Cambridge, MA 02138 \n2. Dept. of Mathematics, Peking University, Beijing, P.R.China \nAbstract \nIn the presence of outliers, the existing self-organizing rules for \nPrincipal Component Analysis (PCA) perform poorly. Using sta- \ntistical physics techniques including the Gibbs distribution, binary \ndecision fields and effective energies, we propose self-organizing \nPCA rules which are capable of resisting outliers while fulfilling \nvarious PCA-related t\n\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [15,240,450,600]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:5]])\n",
    "\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant topics(top3',top_topics)\n",
    "    print('Paper summary')\n",
    "    print(research_papers[document_number][0:600])\n",
    "    print()\n",
    "\n"
   ]
  }
 ]
}